import onnxruntime as ort
import numpy as np

def load_feedback_model(model_path: str = "models/feedback_model.onnx"):
    """
    Loads the interview feedback generation model in ONNX format, optimized for DirectML
    (which leverages the Snapdragon X Elite NPU where available).

    Parameters
    ----------
    model_path : str
        Path to the ONNX model file.

    Returns
    -------
    onnxruntime.InferenceSession
        A loaded ONNX inference session ready to run.
    """
    return ort.InferenceSession(model_path, providers=["DmlExecutionProvider", "CPUExecutionProvider"])


def generate_feedback(session, question: str, transcript: str) -> str:
    """
    Generates AI feedback using an on-device ONNX model.

    Parameters
    ----------
    session : onnxruntime.InferenceSession
        The loaded ONNX model session.
    question : str
        The interview question.
    transcript : str
        The transcribed response to the question.

    Returns
    -------
    str
        The generated feedback text.
    """
    # Prepare input text (you may adapt this based on how your model expects the input)
    input_text = f"Question: {question}\nResponse: {transcript}"
    
    # Create input array (depends on your tokenizer if you use one)
    input_array = np.array([input_text], dtype=np.object_)

    # Run inference
    input_name = session.get_inputs()[0].name
    output = session.run(None, {input_name: input_array})

    # Extract feedback text
    return output[0][0]  # This depends on the model output shape â€” adapt if necessary


def run_pipeline(transcript: str, question: str, model_path: str = "models/feedback_model.onnx") -> dict:
    """
    Runs the feedback pipeline using an ONNX model optimized for Snapdragon X Elite NPU.

    Parameters
    ----------
    transcript : str
        The transcribed response to the interview question.
    question : str
        The interview question.
    model_path : str
        Path to the ONNX feedback generation model.

    Returns
    -------
    dict
        A dictionary containing:
        - "llm_feedback": Feedback generated by the on-device model.
    """
    # Load model once (you could optimize further by making it a global cached object if reused frequently)
    session = load_feedback_model(model_path)

    # Generate feedback
    feedback = generate_feedback(session, question, transcript)

    # Return structured output
    return {
        "llm_feedback": feedback
    }
